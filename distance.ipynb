{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Edit Distance\n",
    "\n",
    "- [Cosine Similarity, cosine distance explained](https://www.youtube.com/watch?v=m_CooIRM3UI)\n",
    "- [Counting Words with CountVectorizer](https://investigate.ai/text-analysis/counting-words-with-scikit-learns-countvectorizer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "\n",
    "cosine_similarity([[3,1]],[[6,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.11022302e-16]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_distances([[3,1]],[[6,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.34905556, 0.42006978, 0.32807142],\n",
       "       [0.34905556, 1.        , 0.30820565, 0.2397661 ],\n",
       "       [0.42006978, 0.30820565, 1.        , 0.28288798],\n",
       "       [0.32807142, 0.2397661 , 0.28288798, 1.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = \"\"\"\n",
    "As the others have said, don't use the disk cache because of how slow it is. \n",
    "Even lowering the number of GPU layers (which then splits it between GPU VRAM \n",
    "and system RAM) slows it down tremendously. Keeping that in mind, the 13B file \n",
    "is almost certainly too large. Remember that the 13B is a reference to the \n",
    "number of parameters, not the file size.\n",
    "\"\"\"\n",
    "\n",
    "doc2 = \"\"\"\n",
    "Just using OPT-Nerys models as an example (huggingface model repository), 13B \n",
    "is over 25GB, which is too large to split between your GPU and RAM. 6B is 13+GB, \n",
    "so it could be used with a 50/50 split (which leaves a little VRAM for tokens \n",
    "and such) or the 2.7B model which is under 6GB and could be loaded fully into \n",
    "VRAM.\n",
    "\"\"\"\n",
    "\n",
    "doc3 = \"\"\"\n",
    "Don't ignore using system ram. It keeps getting cheaper. 2xDDR3200's, \n",
    "32GB total are under $100. I use 13B models, and usually start with 80% GPU \n",
    "layers, and set disk layers to zero. I get text generation within 5 seconds, \n",
    "so it feels like a human co-writer or chat. I do occasionally get an out of \n",
    "memory, but it's rarely an overall memory problem - just means I have to put \n",
    "less on the GPU (which implies more on the CPU), and that adds a bit of time.\n",
    "\"\"\"\n",
    "\n",
    "doc4 = \"\"\"\n",
    "Just replying to everybody so I can let y'all know that I got it working \n",
    "under 6GB, thanks for the help! I pretty much knew what 13B meant and all \n",
    "that but I wasn't sure about how much 8GB can handle considering with the \n",
    "layer system which was very confusing when setting up (it wasn't explained \n",
    "in a way I could understand lol)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "docmx = {'doc1':[], 'doc2':[], 'doc3':[], 'doc4':[]}\n",
    "counts = {}\n",
    "for i in docmx.keys():\n",
    "    docmx[i] = vectorizer.fit_transform([vars()[i]])\n",
    "    counts[i] = pd.DataFrame(docmx[i].toarray(),\n",
    "                      columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "df = pd.concat(counts, names=['doc', 'word']).reset_index(level=1, drop=True)\n",
    "df.fillna(0, inplace=True)\n",
    "cosine_similarity(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
